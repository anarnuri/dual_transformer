{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-24 00:13:09.191545: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1756008789.202824 2295047 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1756008789.206479 2295047 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1756008789.216468 2295047 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1756008789.216476 2295047 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1756008789.216478 2295047 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1756008789.216479 2295047 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-08-24 00:13:09.219749: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "from model import SingleImageTransformer\n",
    "from dataset import SingleImageTransformerDataset\n",
    "import os \n",
    "import numpy as np\n",
    "import shutil\n",
    "from sim import simulate_mechanism\n",
    "from utils import preprocess_curves\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from contextlib import contextmanager\n",
    "\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Checkpoint loaded from weights/image_weights/d1024_h32_n6_bs1024_lr0.0001_clip1.0_best.pth\n",
      "Checkpoint keys: ['model_state_dict', 'optimizer_state_dict', 'clip_loss_state_dict', 'epoch', 'best_loss', 'batch_size', 'learning_rate', 'model_config', 'global_step']\n",
      "\n",
      "üîç Model vs Checkpoint Key Analysis:\n",
      "Model has 807 keys\n",
      "Checkpoint has 807 keys\n",
      "\n",
      "‚úÖ Model weights loaded successfully!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SingleImageTransformer(\n",
       "  (encoder_positional_encoding): PositionalEncoding(\n",
       "    (dropout): Dropout(p=0.0, inplace=False)\n",
       "  )\n",
       "  (decoder_positional_encoding): PositionalEncoding(\n",
       "    (dropout): Dropout(p=0.0, inplace=False)\n",
       "  )\n",
       "  (tgt_embed): InputEmbeddings(\n",
       "    (embedding): Linear(in_features=2, out_features=1024, bias=False)\n",
       "  )\n",
       "  (encoder): ModuleList(\n",
       "    (0-5): 6 x Encoder(\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "      )\n",
       "      (feed_forward): FeedForwardBlock(\n",
       "        (w1): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "        (w2): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "        (w3): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (attention_norm): RMSNorm()\n",
       "      (ffn_norm): RMSNorm()\n",
       "      (final_norm): RMSNorm()\n",
       "    )\n",
       "  )\n",
       "  (decoder): ModuleList(\n",
       "    (0-5): 6 x Decoder(\n",
       "      (self_attention): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "      )\n",
       "      (cross_attention): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "      )\n",
       "      (self_attention_norm): RMSNorm()\n",
       "      (cross_attention_norm): RMSNorm()\n",
       "      (ffn_norm): RMSNorm()\n",
       "      (final_norm): RMSNorm()\n",
       "      (feed_forward): FeedForwardBlock(\n",
       "        (w1): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "        (w2): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "        (w3): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (projection): Linear(in_features=1024, out_features=2, bias=False)\n",
       "  (projection_norm): RMSNorm()\n",
       "  (contrastive_curve): ContrastiveEncoder(\n",
       "    (convnet): ResNet(\n",
       "      (conv1): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "      (layer1): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer2): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (3): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer3): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (3): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (4): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (5): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer4): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "      (fc): Identity()\n",
       "    )\n",
       "    (projector): ProjectionHead(\n",
       "      (fc1): Linear(in_features=2048, out_features=1024, bias=False)\n",
       "      (rm1): RMSNorm()\n",
       "      (act1): GELU(approximate='none')\n",
       "      (fc2): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "      (rm2): RMSNorm()\n",
       "      (act2): SiLU()\n",
       "      (fc_out): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (contrastive_adj): ContrastiveEncoder(\n",
       "    (convnet): ResNet(\n",
       "      (conv1): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "      (layer1): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer2): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (3): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer3): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (3): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (4): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (5): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer4): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "      (fc): Identity()\n",
       "    )\n",
       "    (projector): ProjectionHead(\n",
       "      (fc1): Linear(in_features=2048, out_features=1024, bias=False)\n",
       "      (rm1): RMSNorm()\n",
       "      (act1): GELU(approximate='none')\n",
       "      (fc2): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "      (rm2): RMSNorm()\n",
       "      (act2): SiLU()\n",
       "      (fc_out): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def load_ddp_model(model, checkpoint_path, strict=True, verbose=True):\n",
    "    \"\"\"\n",
    "    Load model weights from checkpoint, handling DDP wrapping if present.\n",
    "    \n",
    "    Args:\n",
    "        model: The model to load weights into\n",
    "        checkpoint_path: Path to the checkpoint file\n",
    "        clip_loss_fn: Optional CLIP loss function to load weights for\n",
    "        strict: Whether to strictly enforce matching keys between model and checkpoint\n",
    "        verbose: Whether to print debugging information\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (model, checkpoint) with loaded weights\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load checkpoint with error handling\n",
    "        checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "        if verbose:\n",
    "            print(f\"‚úÖ Checkpoint loaded from {checkpoint_path}\")\n",
    "            print(f\"Checkpoint keys: {list(checkpoint.keys())}\")\n",
    "            # print(checkpoint['model_state_dict'])\n",
    "        \n",
    "        # Extract state dict\n",
    "        state_dict = checkpoint.get(\"model_state_dict\", checkpoint)\n",
    "        \n",
    "        # Remove DDP wrapper prefixes\n",
    "        new_state_dict = {}\n",
    "        for k, v in state_dict.items():\n",
    "            new_key = k.replace(\"module.\", \"\") if k.startswith(\"module.\") else k\n",
    "            new_state_dict[new_key] = v\n",
    "            if verbose and k != new_key:\n",
    "                print(f\"‚ö†Ô∏è Renamed DDP key: {k} -> {new_key}\")\n",
    "        \n",
    "        # Verify model architecture matches checkpoint\n",
    "        model_keys = set(model.state_dict().keys())\n",
    "        ckpt_keys = set(new_state_dict.keys())\n",
    "        \n",
    "        if verbose:\n",
    "            print(\"\\nüîç Model vs Checkpoint Key Analysis:\")\n",
    "            print(f\"Model has {len(model_keys)} keys\")\n",
    "            print(f\"Checkpoint has {len(ckpt_keys)} keys\")\n",
    "            \n",
    "            missing_in_model = ckpt_keys - model_keys\n",
    "            missing_in_ckpt = model_keys - ckpt_keys\n",
    "            \n",
    "            if missing_in_model:\n",
    "                print(f\"\\n‚ùå Keys in checkpoint but not in model: {missing_in_model}\")\n",
    "            if missing_in_ckpt:\n",
    "                print(f\"\\n‚ùå Keys in model but not in checkpoint: {missing_in_ckpt}\")\n",
    "        \n",
    "        # Load model weights with error handling\n",
    "        try:\n",
    "            model.load_state_dict(new_state_dict, strict=strict)\n",
    "            if verbose:\n",
    "                print(\"\\n‚úÖ Model weights loaded successfully!\")\n",
    "        except RuntimeError as e:\n",
    "            print(f\"\\n‚ùå Error loading model weights: {str(e)}\")\n",
    "            if not strict:\n",
    "                print(\"‚ö†Ô∏è Attempting partial load (strict=False)\")\n",
    "                model.load_state_dict(new_state_dict, strict=False)\n",
    "        \n",
    "        return model, checkpoint\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed to load checkpoint: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "# Model Configuration\n",
    "model_config = {\n",
    "    'output_size': 2,\n",
    "    'tgt_seq_len': 10,\n",
    "    'd_model': 1024,\n",
    "    'h': 32,\n",
    "    'N': 6\n",
    "}\n",
    "# Hyperparameters\n",
    "batch_size = 1024\n",
    "lr = 1e-4\n",
    "\n",
    "# Initialize Model\n",
    "model = SingleImageTransformer(\n",
    "    output_size=model_config['output_size'],\n",
    "    tgt_seq_len=model_config['tgt_seq_len'],\n",
    "    d_model=model_config['d_model'],\n",
    "    h=model_config['h'],\n",
    "    N=model_config['N']\n",
    ").to(device)\n",
    "\n",
    "\n",
    "model, checkpoint = load_ddp_model(model, f\"weights/image_weights/d{model_config['d_model']}_h{model_config['h']}_n{model_config['N']}_bs{batch_size}_lr{lr}_clip1.0_best.pth\")\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = SingleImageTransformerDataset(\n",
    "        data_dir='/home/anurizada/Documents/nobari_10_transformer',\n",
    "    )\n",
    "data_loader = DataLoader(dataset, shuffle=True, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_decode_conditional(model, source, adj_type, max_len, eos_token=torch.tensor([1.0, 1.0])):\n",
    "    model, source = model.to(device), source.to(device)\n",
    "    adj_type = adj_type.to(device)\n",
    "\n",
    "    # Encode the source once\n",
    "    encoder_output, curve_emb, adj_emb = model.encode(source, adj_type)\n",
    "\n",
    "    # Initialize decoder input with start token\n",
    "    decoder_input = torch.ones(1, 1, 2).to(device) * -2.0  # Start token\n",
    "    \n",
    "    while decoder_input.size(1) < max_len:\n",
    "        # Build causal mask (0 for allowed, -inf for blocked)\n",
    "        decoder_mask = causal_mask(decoder_input.size(1), device)\n",
    "\n",
    "        # Decode\n",
    "        decoder_output = model.decode(\n",
    "            encoder_output,\n",
    "            None,\n",
    "            decoder_input,\n",
    "            decoder_mask\n",
    "        )\n",
    "        \n",
    "        proj_output = model.projection(model.projection_norm(decoder_output))\n",
    "        \n",
    "        # Get next token\n",
    "        next_token = proj_output[:, -1].unsqueeze(1)\n",
    "        \n",
    "        # EOS check\n",
    "        if torch.allclose(next_token.squeeze(), eos_token.to(device), atol=1e-1):\n",
    "            break\n",
    "\n",
    "        # Append token\n",
    "        decoder_input = torch.cat([decoder_input, next_token], dim=1)\n",
    "\n",
    "    return decoder_input.squeeze(0), curve_emb, adj_emb\n",
    "\n",
    "\n",
    "def causal_mask(size, device):\n",
    "    # (n, n) upper-triangular boolean mask\n",
    "    mask = torch.triu(torch.ones(size, size, device=device), diagonal=1).bool()\n",
    "    # Add batch dimension -> (1, n, n)\n",
    "    return mask.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 54507 unique adjacency matrices\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2257251 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 64, 64]) torch.Size([1, 1, 20, 20])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/2257251 [00:00<442:15:05,  1.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 64, 64]) torch.Size([1, 1, 20, 20])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2/2257251 [00:00<264:33:41,  2.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 64, 64]) torch.Size([1, 1, 20, 20])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 3/2257251 [00:01<220:40:31,  2.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 64, 64]) torch.Size([1, 1, 20, 20])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 4/2257251 [00:01<190:30:08,  3.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 64, 64]) torch.Size([1, 1, 20, 20])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 5/2257251 [00:01<191:32:43,  3.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 64, 64]) torch.Size([1, 1, 20, 20])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 6/2257251 [00:02<195:36:20,  3.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 64, 64]) torch.Size([1, 1, 20, 20])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 7/2257251 [00:02<192:41:18,  3.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 64, 64]) torch.Size([1, 1, 20, 20])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 8/2257251 [00:02<176:50:50,  3.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 64, 64]) torch.Size([1, 1, 20, 20])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 9/2257251 [00:02<168:40:24,  3.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 64, 64]) torch.Size([1, 1, 20, 20])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 10/2257251 [00:03<173:01:13,  3.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 64, 64]) torch.Size([1, 1, 20, 20])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 11/2257251 [00:03<174:43:36,  3.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 64, 64]) torch.Size([1, 1, 20, 20])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 12/2257251 [00:03<177:27:12,  3.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 64, 64]) torch.Size([1, 1, 20, 20])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 13/2257251 [00:03<169:34:09,  3.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 64, 64]) torch.Size([1, 1, 20, 20])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 14/2257251 [00:04<164:08:36,  3.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 64, 64]) torch.Size([1, 1, 20, 20])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 15/2257251 [00:04<172:37:09,  3.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 64, 64]) torch.Size([1, 1, 20, 20])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 16/2257251 [00:04<166:09:28,  3.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 64, 64]) torch.Size([1, 1, 20, 20])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 17/2257251 [00:04<164:20:29,  3.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 64, 64]) torch.Size([1, 1, 20, 20])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 18/2257251 [00:05<170:26:17,  3.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 64, 64]) torch.Size([1, 1, 20, 20])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 19/2257251 [00:05<166:48:40,  3.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 64, 64]) torch.Size([1, 1, 20, 20])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 20/2257251 [00:05<159:57:10,  3.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 64, 64]) torch.Size([1, 1, 20, 20])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 21/2257251 [00:06<169:55:31,  3.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 64, 64]) torch.Size([1, 1, 20, 20])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 22/2257251 [00:06<168:41:29,  3.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 64, 64]) torch.Size([1, 1, 20, 20])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 23/2257251 [00:06<163:41:57,  3.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 64, 64]) torch.Size([1, 1, 20, 20])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 24/2257251 [00:07<227:02:00,  2.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 64, 64]) torch.Size([1, 1, 20, 20])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 25/2257251 [00:07<212:33:13,  2.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 64, 64]) torch.Size([1, 1, 20, 20])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 26/2257251 [00:07<191:09:57,  3.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 64, 64]) torch.Size([1, 1, 20, 20])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 27/2257251 [00:07<177:44:31,  3.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 64, 64]) torch.Size([1, 1, 20, 20])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 28/2257251 [00:08<166:27:38,  3.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 64, 64]) torch.Size([1, 1, 20, 20])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 29/2257251 [00:08<166:17:33,  3.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 64, 64]) torch.Size([1, 1, 20, 20])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 30/2257251 [00:08<157:21:28,  3.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 64, 64]) torch.Size([1, 1, 20, 20])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 31/2257251 [00:08<156:57:01,  3.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 64, 64]) torch.Size([1, 1, 20, 20])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 32/2257251 [00:09<151:19:36,  4.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 64, 64]) torch.Size([1, 1, 20, 20])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 33/2257251 [00:09<153:32:28,  4.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 64, 64]) torch.Size([1, 1, 20, 20])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 34/2257251 [00:09<160:34:45,  3.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 64, 64]) torch.Size([1, 1, 20, 20])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 35/2257251 [00:09<162:19:41,  3.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 64, 64]) torch.Size([1, 1, 20, 20])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 36/2257251 [00:10<159:28:37,  3.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 64, 64]) torch.Size([1, 1, 20, 20])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 37/2257251 [00:10<163:25:07,  3.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 64, 64]) torch.Size([1, 1, 20, 20])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 38/2257251 [00:10<166:05:04,  3.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 64, 64]) torch.Size([1, 1, 20, 20])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 39/2257251 [00:10<165:20:57,  3.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 64, 64]) torch.Size([1, 1, 20, 20])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 40/2257251 [00:11<159:32:46,  3.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 64, 64]) torch.Size([1, 1, 20, 20])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 41/2257251 [00:11<165:34:26,  3.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 64, 64]) torch.Size([1, 1, 20, 20])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 42/2257251 [00:11<168:53:42,  3.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 64, 64]) torch.Size([1, 1, 20, 20])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 43/2257251 [00:12<177:15:05,  3.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 64, 64]) torch.Size([1, 1, 20, 20])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 44/2257251 [00:12<178:02:57,  3.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 64, 64]) torch.Size([1, 1, 20, 20])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 45/2257251 [00:12<179:47:25,  3.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 64, 64]) torch.Size([1, 1, 20, 20])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 46/2257251 [00:12<176:03:51,  3.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 64, 64]) torch.Size([1, 1, 20, 20])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 47/2257251 [00:13<182:51:39,  3.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 64, 64]) torch.Size([1, 1, 20, 20])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 48/2257251 [00:13<175:28:07,  3.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 64, 64]) torch.Size([1, 1, 20, 20])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 49/2257251 [00:13<174:07:27,  3.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 64, 64]) torch.Size([1, 1, 20, 20])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 50/2257251 [00:14<175:46:48,  3.57it/s]\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "class Config:\n",
    "    results_dir = f\"image_d{model_config['d_model']}_h{model_config['h']}_n{model_config['N']}_bs{batch_size}_lr{lr}\"\n",
    "    max_mech_size = 10\n",
    "    num_conditions = 200  # Number of different adjacency conditions to test per curve\n",
    "    prefix_rows = np.array([[0.5, 0.5], [0.6, 0.5]], dtype=np.float32)\n",
    "    plt_style = {\n",
    "        'truth_joints': {'color': 'red', 'marker': 'o', 's': 60, 'label': 'Truth Joints'},\n",
    "        'pred_joints': {'color': 'blue', 'marker': 'x', 's': 60, 'label': 'Pred Joints'},\n",
    "        'truth_curve': {'color': 'magenta', 'linestyle': '-', 'linewidth': 3, 'label': 'Truth Curve'},\n",
    "        'pred_curve': {'color': 'cyan', 'linestyle': '-', 'linewidth': 2, 'label': 'Predicted Curve'},\n",
    "        'cond_curve': {'color': 'green', 'linestyle': '--', 'linewidth': 2, 'label': 'Condition Curve'}\n",
    "    }\n",
    "\n",
    "# Invalid joint values to remove\n",
    "INVALID_JOINTS = np.array([\n",
    "    [-1.0, -1.0],\n",
    "    [1.0,  1.0],\n",
    "], dtype=np.float32)\n",
    "\n",
    "# Setup\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device).float()  # assume `model` is already defined elsewhere\n",
    "\n",
    "@contextmanager\n",
    "def managed_figure(figsize=(10, 8)):\n",
    "    \"\"\"Context manager to ensure figures are properly cleaned up\"\"\"\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    try:\n",
    "        yield ax\n",
    "    finally:\n",
    "        plt.close(fig)\n",
    "\n",
    "def prepare_results_dir():\n",
    "    \"\"\"Prepare clean results directory\"\"\"\n",
    "    if os.path.exists(Config.results_dir):\n",
    "        shutil.rmtree(Config.results_dir)\n",
    "    os.makedirs(Config.results_dir, exist_ok=True)\n",
    "\n",
    "def process_adjacency(adj_tensor):\n",
    "    \"\"\"Convert raw adjacency to simulation format (handles both tensors and numpy arrays)\"\"\"\n",
    "    if isinstance(adj_tensor, torch.Tensor):\n",
    "        adj = adj_tensor.detach().cpu().squeeze().float().numpy()\n",
    "    elif isinstance(adj_tensor, np.ndarray):\n",
    "        adj = adj_tensor.squeeze().astype(np.float32)\n",
    "    else:\n",
    "        raise TypeError(f\"Input must be torch.Tensor or numpy.ndarray, got {type(adj_tensor)}\")\n",
    "\n",
    "    valid_mask = ~np.all(adj == 0, axis=1)\n",
    "    adj = adj[valid_mask][:, valid_mask]\n",
    "    node_types = np.diag(adj).astype(bool)\n",
    "    np.fill_diagonal(adj, 0)\n",
    "    return adj.astype(np.float32), node_types\n",
    "\n",
    "def simulate(adj, joints, node_types):\n",
    "    \"\"\"Run simulation with error handling\"\"\"\n",
    "    try:\n",
    "        if adj.dtype != np.float32:\n",
    "            adj = adj.astype(np.float32)\n",
    "        if joints.dtype != np.float32:\n",
    "            joints = joints.astype(np.float32)\n",
    "        \n",
    "        result = simulate_mechanism(adj, joints, node_types)\n",
    "        if result is None:\n",
    "            return None\n",
    "        \n",
    "        trajectory = torch.tensor(result[-1], dtype=torch.float32).unsqueeze(0)\n",
    "        return preprocess_curves(trajectory).detach().cpu().numpy().squeeze()\n",
    "    except Exception as e:\n",
    "        return None\n",
    "\n",
    "def get_joints(pred_sequence):\n",
    "    \"\"\"Prepare joint positions from single decoder output\"\"\"\n",
    "    # Remove start token and EOS token if present\n",
    "    joints = pred_sequence[1:].detach().cpu().float()\n",
    "    return np.concatenate([Config.prefix_rows, joints.numpy()], axis=0)\n",
    "\n",
    "def save_plot(ax, curve_dir, filename, title=None):\n",
    "    \"\"\"Helper function to save plots with guaranteed cleanup\"\"\"\n",
    "    try:\n",
    "        if title:\n",
    "            ax.set_title(title)\n",
    "        ax.legend()\n",
    "        ax.grid(True)\n",
    "        ax.axis('equal')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(curve_dir, filename))\n",
    "    finally:\n",
    "        plt.close()\n",
    "\n",
    "def filter_invalid_joints(joints: np.ndarray, invalid_values: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Remove any row in `joints` that exactly matches any row in `invalid_values`.\"\"\"\n",
    "    mask = np.ones(len(joints), dtype=bool)\n",
    "    for invalid in invalid_values:\n",
    "        mask &= ~np.all(joints == invalid, axis=1)\n",
    "    return joints[mask]\n",
    "\n",
    "def create_curve_plot(ax, truth_joints, pred_joints, gt_curve=None, pred_curve=None, \n",
    "                     cond_curve=None, title=None):\n",
    "    \"\"\"Create a standardized plot with joint enumeration\"\"\"\n",
    "    try:\n",
    "        if gt_curve is not None:\n",
    "            ax.plot(gt_curve[:, 0], gt_curve[:, 1], **Config.plt_style['truth_curve'])\n",
    "        if pred_curve is not None:\n",
    "            ax.plot(pred_curve[:, 0], pred_curve[:, 1], **Config.plt_style['pred_curve'])\n",
    "        if cond_curve is not None:\n",
    "            ax.plot(cond_curve[:, 0], cond_curve[:, 1], **Config.plt_style['cond_curve'])\n",
    "        \n",
    "        # Scatter points\n",
    "        ax.scatter(truth_joints[:, 0], truth_joints[:, 1], **Config.plt_style['truth_joints'])\n",
    "        ax.scatter(pred_joints[:, 0], pred_joints[:, 1], **Config.plt_style['pred_joints'])\n",
    "\n",
    "        # Annotate truth joints\n",
    "        for idx, (x, y) in enumerate(truth_joints):\n",
    "            ax.annotate(str(idx), (x, y), textcoords=\"offset points\", xytext=(5,5), fontsize=8, color=\"red\")\n",
    "\n",
    "        # Annotate predicted joints\n",
    "        for idx, (x, y) in enumerate(pred_joints):\n",
    "            ax.annotate(str(idx), (x, y), textcoords=\"offset points\", xytext=(5,-10), fontsize=8, color=\"blue\")\n",
    "\n",
    "        if title:\n",
    "            ax.set_title(title)\n",
    "        ax.set_xlabel('X Position')\n",
    "        ax.set_ylabel('Y Position')\n",
    "    except Exception as e:\n",
    "        pass\n",
    "\n",
    "# ---------------------------\n",
    "# Utility functions\n",
    "# ---------------------------\n",
    "def compute_all_adj_embeddings(model, unique_adjs, curve_data, device):\n",
    "    \"\"\"Compute and cache embeddings for all candidate adjacencies.\"\"\"\n",
    "    all_adj_embs = []\n",
    "    with torch.no_grad():\n",
    "        for cond_adj in tqdm(unique_adjs, desc=\"Caching adjacency embeddings\"):\n",
    "            cond_adj_tensor = torch.from_numpy(cond_adj).to(device).float().unsqueeze(0)\n",
    "            _, _, cond_adj_emb = model.encode(curve_data, cond_adj_tensor)\n",
    "\n",
    "            # (1,1,1024) -> (1024,)\n",
    "            cond_adj_emb = cond_adj_emb.squeeze(0).squeeze(0)\n",
    "            all_adj_embs.append(cond_adj_emb)\n",
    "\n",
    "    # [M, 1024]\n",
    "    all_adj_embs = torch.stack(all_adj_embs, dim=0)\n",
    "    all_adj_embs = F.normalize(all_adj_embs, p=2, dim=-1)\n",
    "    return all_adj_embs\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Main processing\n",
    "# ---------------------------\n",
    "prepare_results_dir()\n",
    "unique_adjs = np.load(\"unique_adjacency_matrices.npy\")\n",
    "print(f\"Loaded {len(unique_adjs)} unique adjacency matrices\")\n",
    "\n",
    "for batch_idx, batch in enumerate(tqdm(data_loader)):\n",
    "    if batch_idx == 50:\n",
    "        break\n",
    "\n",
    "    # Create subfolder for this curve\n",
    "    curve_dir = os.path.join(Config.results_dir, f'curve_{batch_idx}')\n",
    "    os.makedirs(curve_dir, exist_ok=True)\n",
    "\n",
    "    # Prepare data\n",
    "    curve_data = batch[\"curve_numerical\"].to(device).float()\n",
    "    gt_adj = batch[\"adjacency\"].to(device).float()\n",
    "\n",
    "    # Get ground truth joints\n",
    "    truth_joints = batch[\"label\"].view(-1, 2)[:-1].detach().cpu().float()  # Remove EOS token\n",
    "    truth_joints = np.concatenate([Config.prefix_rows, truth_joints.numpy()], axis=0)\n",
    "\n",
    "    # Process ground truth adjacency\n",
    "    gt_adj_processed, gt_node_types = process_adjacency(gt_adj)\n",
    "\n",
    "    # Get prediction with ground truth adjacency\n",
    "    with torch.no_grad():\n",
    "        print(curve_data.shape, gt_adj.shape)\n",
    "        pred_sequence, curve_emb, gt_adj_emb = greedy_decode_conditional(\n",
    "            model, curve_data, gt_adj, Config.max_mech_size\n",
    "        )\n",
    "\n",
    "    # (1,1,1024) -> (1,1024)\n",
    "    curve_emb = curve_emb.squeeze(0).squeeze(0).unsqueeze(0)  # [1,1024]\n",
    "    curve_emb = F.normalize(curve_emb, p=2, dim=-1)\n",
    "\n",
    "    # Prediction joints\n",
    "    pred_joints = get_joints(pred_sequence)\n",
    "    pred_joints = filter_invalid_joints(pred_joints, INVALID_JOINTS)\n",
    "    truth_joints = filter_invalid_joints(truth_joints, INVALID_JOINTS)\n",
    "\n",
    "    gt_curve = simulate(gt_adj_processed, truth_joints, gt_node_types)\n",
    "    pred_curve = simulate(gt_adj_processed, pred_joints, gt_node_types)\n",
    "\n",
    "    # Plot GT vs Pred\n",
    "    with managed_figure() as ax:\n",
    "        create_curve_plot(\n",
    "            ax, truth_joints, pred_joints,\n",
    "            gt_curve=gt_curve, pred_curve=pred_curve,\n",
    "            title=f'Curve {batch_idx}: GT vs Pred'\n",
    "        )\n",
    "        save_plot(ax, curve_dir, f'gt_vs_pred_sim.png')\n",
    "\n",
    "    # # ---------------------------\n",
    "    # # Step 2: Compare against all adjacencies\n",
    "    # # ---------------------------\n",
    "    # all_adj_embs = compute_all_adj_embeddings(model, unique_adjs[:Config.num_conditions], curve_data, device)\n",
    "    # # Similarity [1,1024] @ [1024,M] -> [M]\n",
    "    # sims = (curve_emb @ all_adj_embs.T).squeeze(0)\n",
    "\n",
    "    # # Top-k candidates\n",
    "    # topk_vals, topk_idx = sims.topk(100)\n",
    "\n",
    "    # for rank, (sim_val, cond_idx) in enumerate(zip(topk_vals.tolist(), topk_idx.tolist())):\n",
    "    #     cond_adj = unique_adjs[cond_idx]\n",
    "    #     cond_adj_tensor = torch.from_numpy(cond_adj).to(device).float().unsqueeze(0)\n",
    "\n",
    "    #     with torch.no_grad():\n",
    "    #         cond_sequence, _, _ = greedy_decode_conditional(\n",
    "    #             model, curve_data, cond_adj_tensor, Config.max_mech_size\n",
    "    #         )\n",
    "\n",
    "    #     cond_joints = get_joints(cond_sequence)\n",
    "    #     cond_joints = filter_invalid_joints(cond_joints, INVALID_JOINTS)\n",
    "    #     cond_adj_processed, cond_node_types = process_adjacency(cond_adj)\n",
    "    #     cond_curve = simulate(cond_adj_processed, cond_joints, cond_node_types)\n",
    "\n",
    "    #     if cond_curve is None:\n",
    "    #         continue\n",
    "\n",
    "    #     with managed_figure() as ax:\n",
    "    #         create_curve_plot(\n",
    "    #             ax, truth_joints, cond_joints,\n",
    "    #             gt_curve=gt_curve, cond_curve=cond_curve,\n",
    "    #             title=f'Curve {batch_idx} CondRank {rank} (sim={sim_val:.4f})'\n",
    "    #         )\n",
    "    #         save_plot(ax, curve_dir, f'cond_rank_{rank}_sim_{sim_val:.4f}.png')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# import torch\n",
    "# from tqdm import tqdm\n",
    "# import numpy as np\n",
    "# from collections import defaultdict\n",
    "\n",
    "# # Track unique matrices and their counts\n",
    "# unique_matrices = {}  # {hash_key: (matrix, count)}\n",
    "# counts = defaultdict(int)\n",
    "\n",
    "# for batch in tqdm(data_loader):\n",
    "#     gt_adj = batch[\"adjacency\"].float().cpu()  # Original matrices\n",
    "    \n",
    "#     for matrix in gt_adj:\n",
    "#         # Convert to numpy and create hashable key\n",
    "#         np_matrix = matrix.numpy()\n",
    "#         matrix_key = tuple(np_matrix.round(4).flatten())  # Round to avoid float precision issues\n",
    "        \n",
    "#         # Store first occurrence and count\n",
    "#         if matrix_key not in unique_matrices:\n",
    "#             unique_matrices[matrix_key] = np_matrix\n",
    "#         counts[matrix_key] += 1\n",
    "\n",
    "# # Save all unique matrices in one file\n",
    "# all_matrices = np.stack(list(unique_matrices.values()))\n",
    "# np.save(\"unique_adjacency_matrices.npy\", all_matrices)\n",
    "\n",
    "# print(f\"Saved {len(unique_matrices)} unique matrices\")\n",
    "# print(f\"Output shape: {all_matrices.shape}\")  # e.g. (25, 10, 10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
